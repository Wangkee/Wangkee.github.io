<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>数据结构复习</title>
    <link href="/2021/12/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%A4%8D%E4%B9%A0/"/>
    <url>/2021/12/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="1-时间复杂度"><a href="#1-时间复杂度" class="headerlink" title="1.时间复杂度"></a>1.时间复杂度</h2><p>1.设计一个算法，求含$n$个整数元素的序列中前$i(1\leq i\leq n)$个元素的最大值。并分析算法的平均时间复杂度。</p><div class="code-wrapper"><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int fun(int a[], int n, int i)&#123;int j, max &#x3D; a[0];for(int j &#x3D; 1 ; j &lt;&#x3D; i-1 ; j++)&#123;if(a[j] &gt; max) max &#x3D; a[i];&#125;return max;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><script type="math/tex; mode=display">T(n)=\sum_{i=1}^n\frac{1}{n}\times(i-1)=\frac{1}{n}\sum_{i=1}^n(i-1)=\frac{n-1}{2}=O(n)</script><p>最坏复杂度为$O(n)，i=n$<br>最好复杂度为$O(1)，i=1$</p><p>2.分析以下算法的时间复杂度和空间复杂度。</p><div class="code-wrapper"><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void fun(int n)&#123;int s &#x3D; 0,i,j,k;for(int i &#x3D; 0 ; i &lt;&#x3D; n ; i++)&#123;for(int j &#x3D; 0 ; j &lt;&#x3D; i ; j++)&#123;for(int k &#x3D; 0 ; k &lt; j ; k++)&#123;s++;&#125;&#125;&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><script type="math/tex; mode=display">T(n)=\sum_{i=0}^n\sum_{j=0}^i\sum_{k=0}^{j-1}1=\sum_{i=0}^n\sum_{j=0}^ij=\sum_{i=0}^n(0+1+2\dots+i)=\sum_{i=0}^n(\frac{i(i+1)}{2})\\ \ \ \ \ \ \ \ \ \ \ =\frac{1}{2}(\sum_{i=0}^ni^2+\sum_{i=0}^ni)=\frac{1}{2}(\frac{1}{6}n(n+1)(2n+1)+\frac{1}{2}n(n+1))=O(n^3)</script><p>时间复杂度为$O(n^3)$，空间复杂度为$O(1)$</p><p>3.有如下递归算法，调用上述算法的语句为$fun(a,n,0)$，求其时间复杂度和空间复杂度</p><div class="code-wrapper"><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void fun(int a[], int n, int k)&#123;int i;if(k &#x3D;&#x3D; n-1)&#123;for(i &#x3D; 0 ; i &lt; n ; i++)&#123;printf(&quot;%d&quot;,a[i]);&#125;&#125;else&#123;for(int i &#x3D; k ; i &lt; n ; i++)&#123;a[i] &#x3D; a[i]+i*i;&#125;fun(a,n,k+1);&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><p>假设$fun(a,n,k)$的执行时间为$T_1(n,k)$</p><script type="math/tex; mode=display">\ \ \ \ T(n)=T_1(n,0)=n+T_1(n,1)=n+(n-1)+T_1(n,2) \\ \  = \dots=n+(n-1)+\dots+2+T_1(n,n-1)\\= \dots=n+(n-1)+\dots+2+n=O(n^2)</script><p>所以调用$fun(a,n,0)$的时间复杂度为$O(n^2)$</p><p>假设$fun(a,n,k)$的空间为$S_1(n,k)$</p><script type="math/tex; mode=display">S(n)=S_1(n,0)=1+S_1(n,1)=1+1+\dots+1=O(n)</script><p>以调用$fun(a,n,0)$的空间复杂度为$O(n)$</p><h2 id="2-数据存储"><a href="#2-数据存储" class="headerlink" title="2.数据存储"></a>2.数据存储</h2><p>1.若将$n$阶上三角矩阵A按<strong>列</strong>有限顺序压缩存放在一维数组$B[1,…,(n+1)/2]$中，A中第一个非零元素a(1,1)存于B数组的$b_1$,求$i,j$与$k$的对应关系</p><p>$1\sim j-1$列的元素个数为$\frac{(1+j-1)(j-1)}{2}=\frac{j(j-1)}{2}$</p><p>第$j$列有$i$个元素</p><p>所以$k=\frac{j(j-1)}{2}+i$</p><h2 id="3-模式匹配"><a href="#3-模式匹配" class="headerlink" title="3.模式匹配"></a>3.模式匹配</h2><ul><li>next[j] = k表示什么信息？<br>说明模式串t[j]之前有k个字符已经成功匹配，下一次应当从t[k]开始匹配</li><li>next[j]=-1表示什么信息？<br>说明字符串t[j]之前没有任何用于加速匹配的信息，下一趟应从t的开头即j++(j=0)开始匹配</li><li>分析KMP算法的时间复杂度<br>设主串长为n，子串长为m，求next数组的时间复杂度为$O(m)$，KMP算法的平均时间复杂度为$O(n+m)$，最坏时间复杂度为$O(n\times m)$</li></ul><p>1.求字符串t=”ababaaababaa”的next数组值</p><div class="table-container"><table><thead><tr><th style="text-align:center">j</th><th style="text-align:center">0</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">6</th><th style="text-align:center">7</th><th style="text-align:center">8</th><th style="text-align:center">9</th><th style="text-align:center">10</th><th style="text-align:center">11</th></tr></thead><tbody><tr><td style="text-align:center">t[j]</td><td style="text-align:center">a</td><td style="text-align:center">b</td><td style="text-align:center">a</td><td style="text-align:center">b</td><td style="text-align:center">a</td><td style="text-align:center">a</td><td style="text-align:center">a</td><td style="text-align:center">b</td><td style="text-align:center">a</td><td style="text-align:center">b</td><td style="text-align:center">a</td><td style="text-align:center">a</td></tr><tr><td style="text-align:center">next[j]</td><td style="text-align:center">-1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">3</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">5</td></tr></tbody></table></div><h2 id="4-树"><a href="#4-树" class="headerlink" title="4.树"></a>4.树</h2><h3 id="树的性质"><a href="#树的性质" class="headerlink" title="树的性质"></a>树的性质</h3><h6 id="性质1：树中的节点数等于所有节点的度数加1"><a href="#性质1：树中的节点数等于所有节点的度数加1" class="headerlink" title="性质1：树中的节点数等于所有节点的度数加1"></a>性质1：树中的节点数等于所有节点的度数加1</h6><p>1.在一颗度数为$4$的树中，有$20$个度数为$4$的节点，$10$个度数为$3$的节点，$1$个度为$2$的节点，$10$个度为$1$的节点，求该树叶子节点的个数</p><p>设$n$为总节点个数，$n_i$为度为$i(0\leq i \leq m)$的节点个数</p><script type="math/tex; mode=display">\ \ n=n_1+2n_2+3n_3+4n_4+1=10+2+30+80+1=123 \\n_0=n-(n_1+n_2+n_3+n_4)=123-20-10-1-10=82</script><h6 id="性质2：度为-m-的树第-i-层上至多有-m-i-1-个节点"><a href="#性质2：度为-m-的树第-i-层上至多有-m-i-1-个节点" class="headerlink" title="性质2：度为$m$的树第$i$层上至多有$m^{i-1}$个节点"></a>性质2：度为$m$的树第$i$层上至多有$m^{i-1}$个节点</h6><h6 id="性质3：高度为-h-的-m-次树至多有-1-m-m-2-dots-m-h-1-frac-m-k-1-m-1-个节点"><a href="#性质3：高度为-h-的-m-次树至多有-1-m-m-2-dots-m-h-1-frac-m-k-1-m-1-个节点" class="headerlink" title="性质3：高度为$h$的$m$次树至多有$1+m+m^2+\dots+m^{h-1}=\frac{m^k-1}{m-1}$个节点"></a>性质3：高度为$h$的$m$次树至多有$1+m+m^2+\dots+m^{h-1}=\frac{m^k-1}{m-1}$个节点</h6><h6 id="性质4：具有-n-个节点的-m-次树的最小高度为-lceil-log-m-n-m-1-1-rceil"><a href="#性质4：具有-n-个节点的-m-次树的最小高度为-lceil-log-m-n-m-1-1-rceil" class="headerlink" title="性质4：具有$n$个节点的$m$次树的最小高度为$\lceil log_m(n(m-1)+1)\rceil$"></a>性质4：具有$n$个节点的$m$次树的最小高度为$\lceil log_m(n(m-1)+1)\rceil$</h6><p>2.含有$n$个节点的$3$次树的最小高度是多少？最大高度是多少</p><p>设最小高度为$h$，则有</p><script type="math/tex; mode=display">\ \ \ \ 1+3^1+3^2+\cdots+3^{h-2}<n\leq1+3^1+3^2+\cdots+3^{h-1}\\\frac{3^{h-1}-1}{2}<n\leq\frac{3^h-1}{2} \\ 3^{h-1}<2n+1\leq3^h \\h = \lceil log_3(2n+1)\rceil</script><p>某层有$3$个节点，其余层都只有$1$个节点时高度最大为$n-2$</p><h3 id="二叉树的性质"><a href="#二叉树的性质" class="headerlink" title="二叉树的性质"></a>二叉树的性质</h3><h6 id="性质1：非空二叉树上叶子节点的个数等于双分支节点数加1，即-n-0-n-2-1"><a href="#性质1：非空二叉树上叶子节点的个数等于双分支节点数加1，即-n-0-n-2-1" class="headerlink" title="性质1：非空二叉树上叶子节点的个数等于双分支节点数加1，即$n_0=n_2+1$"></a><strong>性质1：非空二叉树上叶子节点的个数等于双分支节点数加1，即$n_0=n_2+1$</strong></h6><p>3.具有$10$个叶子节点的二叉树中有几个度为$2$的节点</p><script type="math/tex; mode=display">n_2=n_0-1=10-1-9</script><h6 id="性质2：非空二叉树上的第-i-层至多有-2-i-1-个节点"><a href="#性质2：非空二叉树上的第-i-层至多有-2-i-1-个节点" class="headerlink" title="性质2：非空二叉树上的第$i$层至多有$2^{i-1}$个节点"></a>性质2：非空二叉树上的第$i$层至多有$2^{i-1}$个节点</h6><h6 id="性质3：高度为-h-的二叉树至多有-1-2-2-2-cdots-2-h-1-2-h-1-个节点"><a href="#性质3：高度为-h-的二叉树至多有-1-2-2-2-cdots-2-h-1-2-h-1-个节点" class="headerlink" title="性质3：高度为$h$的二叉树至多有$1+2+2^2+\cdots+2^{h-1}=2^h-1$个节点"></a>性质3：高度为$h$的二叉树至多有$1+2+2^2+\cdots+2^{h-1}=2^h-1$个节点</h6><p>4.已知一颗完全二叉树的的第$6$层有$8$个叶子节点，求该完全二叉树节点最多和最少分别有多少个</p><p>最少：前$5$层是满二叉树，第$6$层有$8$个节点，节点个数$n_{min}=2^5-1+8=39$</p><p>最多：前$6$层是满二叉树，第$7$层却失了$2\times8$个节点，节点个数$n_{max}=2^7-1-16=111$</p><h6 id="性质4：对完全二叉树层序编号后有如下性质："><a href="#性质4：对完全二叉树层序编号后有如下性质：" class="headerlink" title="性质4：对完全二叉树层序编号后有如下性质："></a>性质4：对完全二叉树层序编号后有如下性质：</h6><ol><li>若$i\leq \lfloor n/2 \rfloor$则$i$为分支节点，否则为叶子节点。</li><li>编号为$i$的节点的双亲节点为$\lfloor i/2 \rfloor$</li><li>若编号为$i$的节点有左孩子，则左孩子节点的编号为$2i$，若有右孩子，则右孩子编号为$2i+1$</li></ol><h6 id="性质6：有-n-个节点的完全二叉树的高度为-lceil-log-2-n-1-rceil"><a href="#性质6：有-n-个节点的完全二叉树的高度为-lceil-log-2-n-1-rceil" class="headerlink" title="性质6：有$n$个节点的完全二叉树的高度为$\lceil log_2(n+1)\rceil$"></a>性质6：有$n$个节点的完全二叉树的高度为$\lceil log_2(n+1)\rceil$</h6><p>5.求一个具有$1025$个节点的二叉树的最小高度和最大高度</p><p>最小：每层都只有一个节点，$h_{max}=1025$</p><p>最大：完全二叉树，$h_{min}=\lceil log_2(1026) \rceil=11$</p><hr><p>6.假设一颗深度为$6$的完全二叉树的第$6$层有$3$个叶子节点，该二叉树共有多少个叶子节点</p><p>该二叉树第$5$层有$2^4=16$个节点，第$6$层的$3$个叶子节点对于第$5$层的$2$个双亲节点，所以第$5$层有$16-2=14$个叶子节点，该二叉树共有$14+3=17$个叶子节点</p><hr><p>7.若一颗完全二叉树有$768$个节点，则该二叉树有多少个叶子节点</p><p>根据$n=n_0+n_1+n_2$和$n_0=n_2+1$得到$n=2n_0+n_1-1$</p><p>完全二叉树的$n_1=0或1$</p><p>$n_1=0$时$n=2n_0-1$，$n_0$不是整数不成立</p><p>$n_1=1$时$n_0=\frac{n}{2}=384$</p><hr><p>8.一颗有$124$个叶子节点的完全二叉树，最多有多少个节点</p><p>根据$n=n_0+n_1+n_2$和$n_0=n_2+1$得$n=124+123+n_1$</p><p>完全二叉树的$n_1=0或1$</p><p>所以$n<em>1=1,n</em>{max}=124+123+1=248$</p><h3 id="二叉树的遍历"><a href="#二叉树的遍历" class="headerlink" title="二叉树的遍历"></a>二叉树的遍历</h3><p>先序遍历：根左右<br>中序遍历：左根右<br>后序遍历：左右根</p><p>同时给定一棵二叉树的先序序列和中序序列或者中序序列和后序序列可以唯一确定二叉树</p><p>同时给定先序序列和后序序列不能唯一确定</p><h3 id="二叉树和森林的相互转换"><a href="#二叉树和森林的相互转换" class="headerlink" title="二叉树和森林的相互转换"></a>二叉树和森林的相互转换</h3><p>森林转二叉树：</p><ol><li>转换：将森林里的每一棵树都转化为二叉树$bt_1,bt_2,…,bt_m$</li><li>连线：将各二叉树的根节点相连</li><li>调整：将$bt_1$的根节点作为整个二叉树的根节点，$bt_2$的根节点作为$bt_1$的右孩子，$bt_3$的根节点作为$bt_2$的右孩子….</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>决策树Decision Tree</title>
    <link href="/2021/12/17/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <url>/2021/12/17/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="决策树-Descision-Tree"><a href="#决策树-Descision-Tree" class="headerlink" title="决策树(Descision Tree)"></a>决策树(Descision Tree)</h1><h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h2><h3 id="决策树-Decision-Tree"><a href="#决策树-Decision-Tree" class="headerlink" title="决策树(Decision Tree)"></a><strong>决策树(Decision Tree)</strong></h3><p>决策树是一种机器学习算法，一棵决策树包含一个根结点，若干内部结点和若干叶子节点；叶子节点对应于决策结果，其他的每个节点对于与一个属性测试；每个节点包括的样本集合根据属性测试结果被划分到子节点中，根节点包含样本全集。<br>A decision tree is a flowchart-like structure in which each internal node represents a “test” on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.</p><h3 id="信息熵-Information-entropy"><a href="#信息熵-Information-entropy" class="headerlink" title="信息熵(Information entropy)"></a><strong>信息熵(Information entropy)</strong></h3><p>信息熵是度量样本集合纯度的一种指标，样本集合$D$中第$k$类样本所占比例为$p_k(k = 1,2,…,|y|)$，则$D$的信息熵定义为：<br>Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations. </p><script type="math/tex; mode=display">H(D) = -\sum_{k=1}^{|y| }p_klog_2p_k</script><p>entropy的值越小，样本<script type="math/tex">D</script>的纯度越高。</p><h3 id="信息增益-Information-gain"><a href="#信息增益-Information-gain" class="headerlink" title="信息增益(Information gain)"></a><strong>信息增益(Information gain)</strong></h3><p>假定属性$a$有$V$个可能的取值${a^1,a^2,…,a^V}$,用$a$对样本$D$进行划分，产生$V$个分支节点，第$v$个分支节点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记作$D^v$，计算出$D^v$的信息熵，并根据样本数赋予权重，与$D$的信息熵做差得到的结果就是信息增益<br>Information Gain is the expected reduction in entropy caused by partitioning the examples according to a given attribute</p><script type="math/tex; mode=display">IG(D,a) = H(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}H(D^v)</script><p>信息增益越大，纯度提升越大，信息增益可以用来进行决策树划分属性时的选择。</p><h3 id="增益率-Information-Gain-ratio"><a href="#增益率-Information-Gain-ratio" class="headerlink" title="增益率(Information Gain ratio)"></a><strong>增益率(Information Gain ratio)</strong></h3><p>信息增益对取值数目较多的属性有所偏好，为了减少这种偏好带来的不利影响C4.5决策树算法采用增益率来划分最优属性：<br>Gain Ratio attempts to lessen the bias of Information Gain on highly branched predictors by introducing a normalizing term called the Intrinsic Information, it is how hard for us to guess in which branch a randomly selected sample is put into.</p><script type="math/tex; mode=display">IGR = \frac{IG(D,a)}{-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}}</script><h3 id="基尼指数-Gini-impurity-Gini-index"><a href="#基尼指数-Gini-impurity-Gini-index" class="headerlink" title="基尼指数(Gini impurity / Gini index)"></a><strong>基尼指数(Gini impurity / Gini index)</strong></h3><p>基尼系数是另一种选择划分属性的度量方式：<br>Gini Index, also known as Gini impurity, calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly. </p><script type="math/tex; mode=display">Gini = 1 - \sum_{k=1}^{|y|}p_k^2</script><p>属性<script type="math/tex">a</script>的基尼指数定义为：</p><script type="math/tex; mode=display">Gini(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script><p>应该选择使得划分后基尼指数最小的属性作为最优划分。</p><h2 id="2-分类树代码实现"><a href="#2-分类树代码实现" class="headerlink" title="2.分类树代码实现"></a>2.分类树代码实现</h2><p>使用sklearn库的DecisionTreeClassifier类实现决策树：</p><h3 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1.导入模块"></a>1.导入模块</h3><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> tree<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_wine<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div><h3 id="2-数据整理"><a href="#2-数据整理" class="headerlink" title="2.数据整理"></a>2.数据整理</h3><p>这里使用sklearn库里自带的红酒分类问题数据集，对其进行简单整理。使用train_test_split方法切割数据，将数据集划分为训练集和测试集，test_size参数指定数据集中的多少数据被划分为测试集。</p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">wine <span class="token operator">=</span> load_wine<span class="token punctuation">(</span><span class="token punctuation">)</span>x <span class="token operator">=</span> wine<span class="token punctuation">.</span>datay <span class="token operator">=</span> wine<span class="token punctuation">.</span>targetx_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></div><h3 id="3-创建决策树"><a href="#3-创建决策树" class="headerlink" title="3.创建决策树"></a>3.创建决策树</h3><p>DecisionTreeClassifier中参数较多，详见官方文档<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier">sklearn.tree.DecisionTreeClassifier — scikit-learn 1.0.1 documentation</a></p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span><span class="token punctuation">)</span>clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><h3 id="4-查看结果"><a href="#4-查看结果" class="headerlink" title="4.查看结果"></a>4.查看结果</h3><p>用predict函数可以获取在测试集上的分类结果，score函数可以计算分类结果的准确性，因为训练集和测试集是随机划分的所以每次的分类结果也不同，用export<em>graphviz函数可以输出文字描述版的决策树，用graphviz库可以画出图形版，也可以用在线graphviz工具绘图Graphviz Online (dreampuf.github.io)，用feature_importances</em>可以查看每个特征对决策树的重要性，进一步地，可以用zip连接特征名和重要性，用元组更直观地显示。</p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>y_test<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''[1 1 1 1 2 1 1 0 1 2 0 1 1 1 1 1 2 2 1 0 0 2 2 1 2 1 2 2 1 0 1 1 0 1 2 0 0 1 1 0 2 2 0 0 1 0 2 2 1 1 2 1 0 1][1 1 1 1 2 1 1 0 1 1 0 1 1 2 1 1 2 2 1 0 1 2 2 1 2 1 2 2 0 0 1 1 0 1 2 0 0 1 1 0 2 2 0 0 1 0 2 2 1 1 2 1 0 1]'''</span><span class="token keyword">print</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''0.8888888888888888'''</span><span class="token keyword">print</span><span class="token punctuation">(</span>tree<span class="token punctuation">.</span>export_graphviz<span class="token punctuation">(</span>clf<span class="token punctuation">,</span> feature_names<span class="token operator">=</span>wine<span class="token punctuation">.</span>feature_names<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>feature_importances_<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''[0.         0.         0.         0.         0.         0. 0.18282092 0.         0.         0.11448317 0.         0.29232129 0.41037462]'''</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token builtin">zip</span><span class="token punctuation">(</span>wine<span class="token punctuation">.</span>feature_names<span class="token punctuation">,</span> clf<span class="token punctuation">.</span>feature_importances_<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''('alcohol', 0.0) ('malic_acid', 0.0) ('ash', 0.0) ('alcalinity_of_ash', 0.02419121930221919) ('magnesium', 0.0) ('total_phenols', 0.0) ('flavanoids', 0.4905897993195419) ('nonflavanoid_phenols', 0.0) ('proanthocyanins', 0.0) ('color_intensity', 0.3325545520401627) ('hue', 0.0) ('od280/od315_of_diluted_wines', 0.0) ('proline', 0.15266442933807614)'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><p><img src="/img/tree.svg" alt=""></p><h3 id="5-减枝优化"><a href="#5-减枝优化" class="headerlink" title="5.减枝优化"></a>5.减枝优化</h3><p>不加限制的情况下，一颗决策树有可能过拟合，在训练集上表现很好，但在测试集上表现糟糕，为了让决策树有更好的泛化性，需要对决策树进行剪枝。在用sklearn的DecisionTreeClassifier生成决策树时，可以加入以下参数进行剪枝</p><h4 id="1-max-depth"><a href="#1-max-depth" class="headerlink" title="1.max_depth"></a>1.max_depth</h4><p>限制树的最大深度，默认none，决策树会生长到每个叶子的节点都是pure为止。<br>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p><h4 id="2-min-samples-leaf"><a href="#2-min-samples-leaf" class="headerlink" title="2.min_samples_leaf"></a>2.min_samples_leaf</h4><p>限制每个叶子节点所包含的最小样本数，如果小于这个数，分支就不会发生，默认值为1。<br>The minimum number of samples required to be at a leaf node.</p><h4 id="3-min-samples-split"><a href="#3-min-samples-split" class="headerlink" title="3.min_samples_split"></a>3.min_samples_split</h4><p>限制分支节点所包含的最小样本数，<br>The minimum number of samples required to split an internal node:</p><h4 id="4-min-impurity-decrease"><a href="#4-min-impurity-decrease" class="headerlink" title="4.min_impurity_decrease"></a>4.min_impurity_decrease</h4><p>限制information gain的大小，小于设定值的分支不会发生，默认值为0。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Mechaine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/12/17/hello-world/"/>
    <url>/2021/12/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
