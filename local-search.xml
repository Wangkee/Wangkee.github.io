<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>决策树Decision Tree</title>
    <link href="/2021/12/17/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <url>/2021/12/17/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="决策树-Descision-Tree"><a href="#决策树-Descision-Tree" class="headerlink" title="决策树(Descision Tree)"></a>决策树(Descision Tree)</h1><h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h2><h3 id="决策树-Decision-Tree"><a href="#决策树-Decision-Tree" class="headerlink" title="决策树(Decision Tree)"></a><strong>决策树(Decision Tree)</strong></h3><p>决策树是一种机器学习算法，一棵决策树包含一个根结点，若干内部结点和若干叶子节点；叶子节点对应于决策结果，其他的每个节点对于与一个属性测试；每个节点包括的样本集合根据属性测试结果被划分到子节点中，根节点包含样本全集。<br>A decision tree is a flowchart-like structure in which each internal node represents a “test” on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.</p><h3 id="信息熵-Information-entropy"><a href="#信息熵-Information-entropy" class="headerlink" title="信息熵(Information entropy)"></a><strong>信息熵(Information entropy)</strong></h3><p>信息熵是度量样本集合纯度的一种指标，样本集合$D$中第$k$类样本所占比例为$p_k(k = 1,2,…,|y|)$，则$D$的信息熵定义为：<br>Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations. </p><script type="math/tex; mode=display">H(D) = -\sum_{k=1}^{|y| }p_klog_2p_k</script><p>entropy的值越小，样本<script type="math/tex">D</script>的纯度越高。</p><h3 id="信息增益-Information-gain"><a href="#信息增益-Information-gain" class="headerlink" title="信息增益(Information gain)"></a><strong>信息增益(Information gain)</strong></h3><p>假定属性$a$有$V$个可能的取值${a^1,a^2,…,a^V}$,用$a$对样本$D$进行划分，产生$V$个分支节点，第$v$个分支节点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记作$D^v$，计算出$D^v$的信息熵，并根据样本数赋予权重，与$D$的信息熵做差得到的结果就是信息增益<br>Information Gain is the expected reduction in entropy caused by partitioning the examples according to a given attribute</p><script type="math/tex; mode=display">IG(D,a) = H(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}H(D^v)</script><p>信息增益越大，纯度提升越大，信息增益可以用来进行决策树划分属性时的选择。</p><h3 id="增益率-Information-Gain-ratio"><a href="#增益率-Information-Gain-ratio" class="headerlink" title="增益率(Information Gain ratio)"></a><strong>增益率(Information Gain ratio)</strong></h3><p>信息增益对取值数目较多的属性有所偏好，为了减少这种偏好带来的不利影响C4.5决策树算法采用增益率来划分最优属性：<br>Gain Ratio attempts to lessen the bias of Information Gain on highly branched predictors by introducing a normalizing term called the Intrinsic Information, it is how hard for us to guess in which branch a randomly selected sample is put into.</p><script type="math/tex; mode=display">IGR = \frac{IG(D,a)}{-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}}</script><h3 id="基尼指数-Gini-impurity-Gini-index"><a href="#基尼指数-Gini-impurity-Gini-index" class="headerlink" title="基尼指数(Gini impurity / Gini index)"></a><strong>基尼指数(Gini impurity / Gini index)</strong></h3><p>基尼系数是另一种选择划分属性的度量方式：<br>Gini Index, also known as Gini impurity, calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly. </p><script type="math/tex; mode=display">Gini = 1 - \sum_{k=1}^{|y|}p_k^2</script><p>属性<script type="math/tex">a</script>的基尼指数定义为：</p><script type="math/tex; mode=display">Gini(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script><p>应该选择使得划分后基尼指数最小的属性作为最优划分。</p><h2 id="2-分类树代码实现"><a href="#2-分类树代码实现" class="headerlink" title="2.分类树代码实现"></a>2.分类树代码实现</h2><p>使用sklearn库的DecisionTreeClassifier类实现决策树：</p><h3 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1.导入模块"></a>1.导入模块</h3><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> tree<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_wine<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div><h3 id="2-数据整理"><a href="#2-数据整理" class="headerlink" title="2.数据整理"></a>2.数据整理</h3><p>这里使用sklearn库里自带的红酒分类问题数据集，对其进行简单整理。使用train_test_split方法切割数据，将数据集划分为训练集和测试集，test_size参数指定数据集中的多少数据被划分为测试集。</p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">wine <span class="token operator">=</span> load_wine<span class="token punctuation">(</span><span class="token punctuation">)</span>x <span class="token operator">=</span> wine<span class="token punctuation">.</span>datay <span class="token operator">=</span> wine<span class="token punctuation">.</span>targetx_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></div><h3 id="3-创建决策树"><a href="#3-创建决策树" class="headerlink" title="3.创建决策树"></a>3.创建决策树</h3><p>DecisionTreeClassifier中参数较多，详见官方文档<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier">sklearn.tree.DecisionTreeClassifier — scikit-learn 1.0.1 documentation</a></p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span><span class="token punctuation">)</span>clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><h3 id="4-查看结果"><a href="#4-查看结果" class="headerlink" title="4.查看结果"></a>4.查看结果</h3><p>用predict函数可以获取在测试集上的分类结果，score函数可以计算分类结果的准确性，因为训练集和测试集是随机划分的所以每次的分类结果也不同，用export<em>graphviz函数可以输出文字描述版的决策树，用graphviz库可以画出图形版，也可以用在线graphviz工具绘图Graphviz Online (dreampuf.github.io)，用feature_importances</em>可以查看每个特征对决策树的重要性，进一步地，可以用zip连接特征名和重要性，用元组更直观地显示。</p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>y_test<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''[1 1 1 1 2 1 1 0 1 2 0 1 1 1 1 1 2 2 1 0 0 2 2 1 2 1 2 2 1 0 1 1 0 1 2 0 0 1 1 0 2 2 0 0 1 0 2 2 1 1 2 1 0 1][1 1 1 1 2 1 1 0 1 1 0 1 1 2 1 1 2 2 1 0 1 2 2 1 2 1 2 2 0 0 1 1 0 1 2 0 0 1 1 0 2 2 0 0 1 0 2 2 1 1 2 1 0 1]'''</span><span class="token keyword">print</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''0.8888888888888888'''</span><span class="token keyword">print</span><span class="token punctuation">(</span>tree<span class="token punctuation">.</span>export_graphviz<span class="token punctuation">(</span>clf<span class="token punctuation">,</span> feature_names<span class="token operator">=</span>wine<span class="token punctuation">.</span>feature_names<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>feature_importances_<span class="token punctuation">)</span><span class="token triple-quoted-string string">'''[0.         0.         0.         0.         0.         0. 0.18282092 0.         0.         0.11448317 0.         0.29232129 0.41037462]'''</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token builtin">zip</span><span class="token punctuation">(</span>wine<span class="token punctuation">.</span>feature_names<span class="token punctuation">,</span> clf<span class="token punctuation">.</span>feature_importances_<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">'''('alcohol', 0.0) ('malic_acid', 0.0) ('ash', 0.0) ('alcalinity_of_ash', 0.02419121930221919) ('magnesium', 0.0) ('total_phenols', 0.0) ('flavanoids', 0.4905897993195419) ('nonflavanoid_phenols', 0.0) ('proanthocyanins', 0.0) ('color_intensity', 0.3325545520401627) ('hue', 0.0) ('od280/od315_of_diluted_wines', 0.0) ('proline', 0.15266442933807614)'''</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><p><img src="/img/tree.svg" alt=""></p><h3 id="5-减枝优化"><a href="#5-减枝优化" class="headerlink" title="5.减枝优化"></a>5.减枝优化</h3><p>不加限制的情况下，一颗决策树有可能过拟合，在训练集上表现很好，但在测试集上表现糟糕，为了让决策树有更好的泛化性，需要对决策树进行剪枝。在用sklearn的DecisionTreeClassifier生成决策树时，可以加入以下参数进行剪枝</p><h4 id="1-max-depth"><a href="#1-max-depth" class="headerlink" title="1.max_depth"></a>1.max_depth</h4><p>限制树的最大深度，默认none，决策树会生长到每个叶子的节点都是pure为止。<br>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p><h4 id="2-min-samples-leaf"><a href="#2-min-samples-leaf" class="headerlink" title="2.min_samples_leaf"></a>2.min_samples_leaf</h4><p>限制每个叶子节点所包含的最小样本数，如果小于这个数，分支就不会发生，默认值为1。<br>The minimum number of samples required to be at a leaf node.</p><h4 id="3-min-samples-split"><a href="#3-min-samples-split" class="headerlink" title="3.min_samples_split"></a>3.min_samples_split</h4><p>限制分支节点所包含的最小样本数，<br>The minimum number of samples required to split an internal node:</p><h4 id="4-min-impurity-decrease"><a href="#4-min-impurity-decrease" class="headerlink" title="4.min_impurity_decrease"></a>4.min_impurity_decrease</h4><p>限制information gain的大小，小于设定值的分支不会发生，默认值为0。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Mechaine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/12/17/hello-world/"/>
    <url>/2021/12/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
